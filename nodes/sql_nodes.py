"""\nSimple SQL Agent Nodes for LangGraph\nHandles user input, SQL query execution, and response generation\n"""\n\nfrom langchain_ollama import OllamaLLM\nfrom .state_schemas import TimeManagementState\nfrom datetime import datetime\nimport json\nimport os\n\n\n# Initialize LLM (local Ollama) - fallback to mock if not available\ndef get_llm():\n    """Get LLM instance, with fallback to mock"""\n    try:\n        return OllamaLLM(model="gpt-oss:20b-cloud", temperature=0.1)\n    except:\n        # Fallback to mock LLM for testing\n        class MockLLM:\n            def invoke(self, prompt: str) -> str:\n                # Simple mock responses\n                if "CREATE TABLE" in prompt.upper():\n                    return json.dumps({\n                        "intent": "create",\n                        "sql_query": "CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT, email TEXT)",\n                        "explanation": "Creating users table"\n                    })\n                elif "INSERT" in prompt.upper():\n                    return json.dumps({\n                        "intent": "insert",\n                        "sql_query": "INSERT INTO users (name, email) VALUES ('John Doe', 'john@example.com')",\n                        "explanation": "Inserting test user"\n                    })\n                elif "SELECT" in prompt.upper() or "SHOW" in prompt.upper():\n                    return json.dumps({\n                        "intent": "select",\n                        "sql_query": "SELECT * FROM sqlite_master WHERE type='table'",\n                        "explanation": "Showing all tables"\n                    })\n                else:\n                    return json.dumps({\n                        "intent": "select",\n                        "sql_query": "SELECT 1",\n                        "explanation": "Default query"\n                    })\n        return MockLLM()\n\nllm = get_llm()\n\n\ndef user_input_node(state: TimeManagementState) -> TimeManagementState:\n    """\n    Node 1: Receives and processes user input.\n    Just stores the current input in state.\n    """\n    action = "user_input_received"\n    state["action_taken"] = action\n    state["last_sync_with_db"] = datetime.now().isoformat()\n    print(f"User Input: {state.get('current_input', 'No input')}")\n    return state\n\n\ndef classify_intent_node(state: TimeManagementState) -> TimeManagementState:\n    """\n    Node 2: Classify user intent and generate SQL query.\n    Returns decision on which SQL operation to perform.\n    """\n    user_input = state.get("current_input", "")\n    prompt = f\"\"\"You are a SQL query assistant. Analyze this user request and generate appropriate SQL.\nUser Request: \"{user_input}\"\nRespond in this JSON format only:\n{{\"intent\": \"select|insert|update|delete|create\", \"sql_query\": \"THE EXACT SQL QUERY\", \"explanation\": \"Brief explanation\"}}\nIMPORTANT: Only generate valid SQL queries. Always return valid JSON.\n\"\"\"\n    response = llm.invoke(prompt)\n    print(f\"LLM Response:\n{response}\n\")\n    try:\n        parsed = json.loads(response)\n        state[\"reasoning\"] = parsed.get(\"explanation\", \"\")\n        state[\"next_step\"] = \"execute_sql\"\n        state[\"action_taken\"] = \"classified_intent\"\n        state[\"sql_query\"] = parsed.get(\"sql_query\", \"\")\n        state[\"intent\"] = parsed.get(\"intent\", \"select\")\n        return state\n    except json.JSONDecodeError:\n        print(f\"Failed to parse LLM response\")\n        state[\"action_taken\"] = \"failed_to_parse_intent\"\n        state[\"next_step\"] = \"handle_error\"\n        state[\"sql_query\"] = \"\"\n        state[\"intent\"] = \"error\"\n        return state\n\n\ndef execute_sql_node(state: TimeManagementState) -> TimeManagementState:\n    """\n    Node 3: Execute the SQL query via MCP tools.\n    This node will be called by Chainlit with actual MCP session.\n    """\n    sql_query = state.get(\"sql_query\", \"\")\n    if not sql_query:\n        state[\"action_taken\"] = \"no_sql_query\"\n        state[\"reasoning\"] = \"No valid SQL query was generated\"\n        return state\n    print(f\"Executing SQL: {sql_query}\")\n    state[\"action_taken\"] = \"sql_executed\"\n    state[\"is_db_updated\"] = True\n    state[\"last_sync_with_db\"] = datetime.now().isoformat()\n    state[\"sql_result\"] = f\"SQL executed: {sql_query[:50]}...\"\n    return state\n\n\ndef generate_response_node(state: TimeManagementState) -> TimeManagementState:\n    """\n    Node 4: Generate user-friendly response from SQL result.\n    """\n    sql_result = state.get(\"sql_result\", \"No result\")\n    user_input = state.get(\"current_input\", \"\")\n    prompt = f\"\"\"Based on this SQL result, provide a friendly response to the user's request.\nUser Asked: \"{user_input}\"\nSQL Result: {sql_result}\nProvide a concise, user-friendly response:\n\"\"\"\n    response = llm.invoke(prompt)\n    print(f\"Generated Response:\n{response}\n\")\n    messages = state.get(\"messages\", [])\n    messages.append({\"role\": \"user\", \"content\": user_input})\n    messages.append({\"role\": \"assistant\", \"content\": response})\n    state[\"messages\"] = messages\n    state[\"action_taken\"] = \"response_generated\"\n    return state\n